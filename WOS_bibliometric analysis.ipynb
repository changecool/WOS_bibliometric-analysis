{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Load Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import csv\n",
    "import os\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import textwrap\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the \"Plain Text File\" exported from WOS as WOS_1.txt and WOS_2.txt, then move them to the same folder as this file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the txt file and test the number of citations read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_records(contents):\n",
    "    # Initialize the count of records\n",
    "    record_count = 0\n",
    "    # Process each file content\n",
    "    for content in contents:\n",
    "        # Convert the list to a string and split by line\n",
    "        lines = ''.join(content).split('\\n')\n",
    "        # Count all lines starting with \"ER\" and add to the record count\n",
    "        record_count += sum(1 for line in lines if line.startswith('ER'))\n",
    "    # Return the count of records\n",
    "    return record_count\n",
    "\n",
    "# Read and count the number of records in two txt files\n",
    "with open('WOS_1.txt', 'r', encoding='utf-8-sig') as file1:\n",
    "    content1 = file1.readlines()\n",
    "with open('WOS_2.txt', 'r', encoding='utf-8-sig') as file2:\n",
    "    content2 = file2.readlines()\n",
    "# Combine the contents of the two files and count the records\n",
    "contents = [content1, content2]\n",
    "print(f\"The combined txt files contain {count_records(contents)} records.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Data Cleaning: Replacing Keywords, Authors, and Country Names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Modifying Author Keywords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Extract Author Keywords: Extract the lines in the txt file that contain the DE field (author keywords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(contents):\n",
    "    # Initialize the keyword list\n",
    "    keywords = []\n",
    "    # Process each file content\n",
    "    for content in contents:\n",
    "        # Convert the list to a string and split by line\n",
    "        lines = ''.join(content).split('\\n')\n",
    "        # Initialize a flag indicating whether we are processing a keyword line\n",
    "        in_keywords = False\n",
    "        # Iterate over all lines\n",
    "        for line in lines:\n",
    "            # If the line starts with \"DE\", add the line (removing the \"DE\" at the beginning) to the keyword list\n",
    "            if line.startswith('DE '):\n",
    "                keywords.append(line[3:])\n",
    "                in_keywords = True\n",
    "            # If the line does not start with \"DE\", starts with a space, but the previous line was a keyword line, append this line (removing the space at the beginning) to the previous line\n",
    "            elif in_keywords and line.startswith(' '):\n",
    "                keywords[-1] += ' ' + line.lstrip()  # Add a space and remove the space at the beginning\n",
    "            else:\n",
    "                in_keywords = False\n",
    "    # Return the keyword list\n",
    "    return keywords\n",
    "\n",
    "# Read and extract keywords from two txt files\n",
    "with open('WOS_1.txt', 'r', encoding='utf-8-sig') as file1:\n",
    "    content1 = file1.readlines()\n",
    "with open('WOS_2.txt', 'r', encoding='utf-8-sig') as file2:\n",
    "    content2 = file2.readlines()\n",
    "# Combine the contents of the two files and extract keywords\n",
    "contents = [content1, content2]\n",
    "keywords = extract_keywords(contents)\n",
    "print(f\"Extracted {len(keywords)} author keywords lines.\")\n",
    "\n",
    "# Print all keywords\n",
    "for keyword in keywords:\n",
    "    print(keyword)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Writing Keywords to CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract keywords and write them into a CSV file\n",
    "def write_keywords_to_csv(keywords, output_file):\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for keyword in keywords:\n",
    "            writer.writerow([keyword])\n",
    "\n",
    "# Use the above function to write keywords into 'wos_keywords.csv' file\n",
    "write_keywords_to_csv(keywords, 'wos_keywords.csv')\n",
    "print(f\"Keywords exported to wos_keywords.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Extracting Unique Values of Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read keywords from a file\n",
    "def read_keywords_from_csv(input_file):\n",
    "    keywords = []\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            keywords.extend(row[0].split(';'))\n",
    "    return keywords\n",
    "\n",
    "# Write keywords to a file\n",
    "def write_keywords_to_csv(keywords, output_file):\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for keyword in keywords:\n",
    "            writer.writerow([keyword])\n",
    "\n",
    "# Read keywords from 'wos_keywords.csv' file\n",
    "keywords = read_keywords_from_csv('wos_keywords.csv')\n",
    "\n",
    "# Remove blank and duplicate keywords, then convert to list format\n",
    "unique_keywords = list(set([k.strip() for k in keywords if k.strip()]))\n",
    "\n",
    "# Calculate the number of unique keywords\n",
    "print(f\"The number of unique keywords: {len(unique_keywords)}\")\n",
    "\n",
    "# Write unique keywords to 'wos_unique_keywords.csv' file\n",
    "write_keywords_to_csv(unique_keywords, 'wos_unique_keywords.csv')\n",
    "print(f\"Unique keywords exported to wos_unique_keywords.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 Manually verify similar keywords and create a replacement CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a file named \"replacefile_authorkeywords.csv\" with two columns: \"Original Keyword\" and \"Processed Keyword\".\n",
    "- The column \"Original Keyword\" is derived from the file wos_unique_keywords.csv, while the column \"Processed Keyword\" stores the replaced keywords.\n",
    "- For keywords that do not require any modifications, simply leave the 'Processed Keyword' column blank."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 Replace Keywords and Export a New txt File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the keyword mapping\n",
    "df = pd.read_csv('replacefile_authorkeywords.csv')\n",
    "keyword_mapping = df.set_index('Original Keyword')['Processed Keyword'].replace(np.nan, '', regex=True).to_dict()\n",
    "\n",
    "def replace_keywords(string, keyword_mapping):\n",
    "    keywords = string.split(\";\")\n",
    "    new_keywords = [keyword_mapping.get(k.strip(), k.strip()) if keyword_mapping.get(k.strip(), k.strip()) not in [None, \"\"] else k.strip() for k in keywords]\n",
    "    # Removing duplicate keywords\n",
    "    new_keywords = list(dict.fromkeys(new_keywords))\n",
    "    # Rejoin keywords with semicolon and space\n",
    "    new_string = \"; \".join(new_keywords)\n",
    "    if not new_string.endswith(\" \"): # Ensure the string ends with a space\n",
    "        new_string += \" \"\n",
    "    return new_string\n",
    "\n",
    "# Process the two txt files\n",
    "for filename in ['WOS_1.txt', 'WOS_2.txt']:\n",
    "    with open(filename, 'r', encoding='utf-8-sig') as file:\n",
    "        content = file.readlines()\n",
    "\n",
    "    new_content = []\n",
    "    in_keywords = False\n",
    "    current_line = \"\"\n",
    "\n",
    "    for line in content:\n",
    "        # If the line starts with \"DE\", it is the start of a keyword line\n",
    "        if line.startswith('DE '):\n",
    "            if in_keywords:\n",
    "                # Replace keywords in current line and append to new content\n",
    "                current_line = replace_keywords(current_line, keyword_mapping)\n",
    "                new_content.append(f'DE {current_line}\\n')\n",
    "            # Start a new keyword line\n",
    "            current_line = line[3:].strip()\n",
    "            in_keywords = True\n",
    "        # If the line doesn't start with \"DE\" but is a continuation of a keyword line, append it to the current line\n",
    "        elif in_keywords and line.startswith(' '):\n",
    "            current_line += ' ' + line.strip()\n",
    "        # If the line is not a keyword line, append it to the new content\n",
    "        else:\n",
    "            if in_keywords:\n",
    "                current_line = replace_keywords(current_line, keyword_mapping)\n",
    "                new_content.append(f'DE {current_line}\\n')\n",
    "                current_line = \"\"\n",
    "                in_keywords = False\n",
    "            new_content.append(line)\n",
    "\n",
    "    # Write the new content to a new file\n",
    "    with open('01_updated_' + filename, 'w', encoding='utf-8-sig') as file:\n",
    "        file.writelines(new_content)\n",
    "\n",
    "print(\"Author keywords have been replaced and duplicates removed! The updated file names are: 01_updated_WOS_1.txt and 01_updated_WOS_2.txt.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Modifying the Author's Name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Extracting Author Names: Extract author names based on the AF field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file paths\n",
    "file_paths = [\"01_updated_WOS_1.txt\", \"01_updated_WOS_2.txt\"]\n",
    "\n",
    "# Initialize a list to collect authors\n",
    "authors_from_af_corrected_multiline = []\n",
    "\n",
    "# Loop through the files and extract authors from the AF field\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r', encoding='utf-8-sig', errors='ignore') as file:\n",
    "        in_af_field = False\n",
    "        for line in file:\n",
    "            if line.startswith(\"AF\"):\n",
    "                in_af_field = True\n",
    "                authors_from_af_corrected_multiline.append(line[3:].strip())\n",
    "            elif in_af_field and line.startswith(\" \"):\n",
    "                authors_from_af_corrected_multiline.append(line.strip())\n",
    "            elif in_af_field and not line.startswith(\" \"):\n",
    "                # Exiting the AF field\n",
    "                in_af_field = False\n",
    "\n",
    "# Counting the publication count for each author\n",
    "author_publication_count_af_multiline = Counter(authors_from_af_corrected_multiline)\n",
    "\n",
    "# Creating a DataFrame from the author publication counts\n",
    "author_df_af_multiline = pd.DataFrame(list(author_publication_count_af_multiline.items()), columns=['Author', 'Publication Count'])\n",
    "\n",
    "# Sorting by publication count\n",
    "author_df_af_multiline = author_df_af_multiline.sort_values(by='Publication Count', ascending=False)\n",
    "\n",
    "# Exporting to CSV\n",
    "csv_file_path_af_multiline = \"wos_authornames_original.csv\"\n",
    "author_df_af_multiline.to_csv(csv_file_path_af_multiline, index=False)\n",
    "\n",
    "print(f\"The original author's name has been saved to {csv_file_path_af_multiline}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Manual Verification of Author Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a file called \"replacefile_authornames.csv\" with two columns, namely \"Author\" and \"Replace by\".\n",
    "- The content of the \"Author\" column comes from the resource \"wos_authornames_original.csv\", and the \"replace by\" column contains replacement content."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Replace the Author's Name and Export It as a New txt File."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the replacement file\n",
    "replace_df = pd.read_csv('replacefile_authornames.csv')\n",
    "# Create a dictionary with 'Author' as key and 'replace by' as value\n",
    "replace_dict = replace_df.set_index('Author')['replace by'].to_dict()\n",
    "\n",
    "# Iterate and modify files\n",
    "for filename in ['01_updated_WOS_1.txt', '01_updated_WOS_2.txt']:\n",
    "    # Open the file in read mode\n",
    "    with open(filename, 'r', encoding='utf-8-sig') as file:\n",
    "        # Read all lines from the file\n",
    "        content = file.readlines()\n",
    "\n",
    "    new_content = []\n",
    "    in_af_line = False\n",
    "    # Iterate over each line in the content\n",
    "    for line in content:\n",
    "        # Check if the line starts with 'AF'\n",
    "        if line.startswith('AF'):\n",
    "            in_af_line = True\n",
    "            # Get the author name starting from the 3rd character\n",
    "            author = line[3:].strip()\n",
    "            # Get the new author name from the dictionary, if not found use the original author name\n",
    "            new_author = replace_dict.get(author, author)\n",
    "            # Append the new author name to the new content\n",
    "            new_content.append('AF ' + new_author + '\\n')\n",
    "        # Check the condition for the end of the 'AF' field\n",
    "        elif in_af_line and not line.startswith(' '):\n",
    "            in_af_line = False\n",
    "            new_content.append(line)\n",
    "        elif in_af_line:\n",
    "            # Remove leading and trailing spaces from the line\n",
    "            author = line.strip()\n",
    "            # Check if the author needs to be replaced\n",
    "            new_author = replace_dict.get(author, author)\n",
    "            # Append the new content with spaces at the beginning\n",
    "            new_content.append('   ' + new_author + '\\n')\n",
    "        else:\n",
    "            new_content.append(line)\n",
    "\n",
    "    # Write to a new file, the new filename is the original filename with a '_updated' suffix\n",
    "    new_filename = '02_' + filename.replace('01_', '') \n",
    "    with open(new_filename, 'w', encoding='utf-8-sig') as file:\n",
    "        # Write all lines to the file\n",
    "        file.writelines(new_content)\n",
    "\n",
    "print(\"The author's name has been replaced successfully, and the new file is saved as 02_updated_WOS_1.txt and 02_updated_WOS_1.txt!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Modifying Country Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Manually Creating a Country Name Replacement File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a file named \"replacefile_countrynames.csv\" with two columns: one column for the \"Country\" and another column for the \"replace by\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Replace country names and export as a new txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read replacement content from CSV file\n",
    "replace_dict = {}\n",
    "with open('replacefile_countrynames.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        replace_dict[row[0]] = row[1]\n",
    "\n",
    "# Read the original file and write the replaced content to a new file\n",
    "input_files = ['02_updated_WOS_1.txt', '02_updated_WOS_2.txt']\n",
    "for input_file in input_files:\n",
    "    output_file = '03_' + input_file.replace('02_', '')\n",
    "    with open(input_file, 'r', encoding='utf-8-sig') as infile, open(output_file, 'w', encoding='utf-8-sig') as outfile:\n",
    "        in_c1_section = False\n",
    "        for line in infile:\n",
    "            if line.startswith('C1'):\n",
    "                in_c1_section = True\n",
    "            elif not line.startswith(' '):\n",
    "                in_c1_section = False\n",
    "                \n",
    "            # Replace the country name in the C1 section\n",
    "            if in_c1_section:\n",
    "                for original, replacement in replace_dict.items():\n",
    "                    line = line.replace(original, replacement)\n",
    "            outfile.write(line)\n",
    "\n",
    "print(\"The country name has been successfully replaced, and the new file is saved as 03_updated_WOS_1.txt and 03_updated_WOS_1.txt!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Correction of citation txt errors (Example, please adjust the code according to the actual situation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When comparing the analysis results of VOSviewer, Python code, and Web of Science (WoS) data retrieval, it was found that both VOSviewer and Python code reported one less publication count for articles from the United Kingdom compared to the analysis results obtained from WoS data retrieval. After careful manual verification, it was discovered that this discrepancy was due to an irregular format in the C1 field line of the citation file for the article titled \"Select Rab GTPases Regulate the Pulmonary Endothelium via Endosomal Trafficking of Vascular Endothelial-Cadherin,\" which did not include the contact address of the corresponding author.\n",
    "- To rectify this issue, we manually corrected the C1 field line in the citation file for \"Select Rab GTPases Regulate the Pulmonary Endothelium via Endosomal Trafficking of Vascular Endothelial-Cadherin\" by adding the contact address of the corresponding author. This correction ensures that there is no bias in our analysis when using VOSviewer or Python code.\n",
    "- Furthermore, to prevent similar errors from occurring again in future analyses, we have marked all C1 field lines in our text files and used Python code to retrieve all non-standard format lines. These lines were then manually verified to confirm whether they included contact addresses for corresponding authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new line to the file after the specified line number\n",
    "def insert_line(filename, line_num, text):\n",
    "    with open(filename, 'r', encoding='utf-8-sig') as file:\n",
    "        content = file.readlines()\n",
    "    content.insert(line_num, text + '\\n')  # Insert the new line\n",
    "    return content\n",
    "\n",
    "# Save the content to a new file\n",
    "def save_to_file(filename, content):\n",
    "    with open(filename, 'w', encoding='utf-8-sig') as file:\n",
    "        file.writelines(content)\n",
    "\n",
    "# Add the new line to 03_updated_WOS_2.txt and save as 05_final_WOS_2.txt\n",
    "new_line = \"   Anglia Ruskin Univ, Biomed Res Grp, Dept Life Sci, Fac Sci & Technol, OPT 024,East Rd, Cambridge CB1 1PT, England, United Kingdom.\"\n",
    "updated_content = insert_line('03_updated_WOS_2.txt', 19691, new_line)\n",
    "save_to_file('05_final_WOS_2.txt', updated_content)\n",
    "\n",
    "# Save 03_updated_WOS_1.txt as 05_final_WOS_1.txt without any changes\n",
    "with open('03_updated_WOS_1.txt', 'r', encoding='utf-8-sig') as file:\n",
    "    content = file.readlines()\n",
    "save_to_file('05_final_WOS_1.txt', content)\n",
    "\n",
    "print(\"Files saved as 05_final_WOS_1.txt and 05_final_WOS_2.txt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Create txt files with all C1 field rows starting with C1: 04_C1updated_WOS_1.txt and 04_C1updated_WOS_2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_c1_lines(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    in_c1_section = False\n",
    "    modified_lines = []\n",
    "    for line in lines:\n",
    "        if line.startswith(\"C1\"):\n",
    "            in_c1_section = True\n",
    "        elif not line.startswith(\" \") and in_c1_section:\n",
    "            in_c1_section = False\n",
    "\n",
    "        if in_c1_section and line.startswith(\" \"):\n",
    "            # Replace lines beginning with spaces with lines that begin with \"C1\" and ensure that spaces are preserved\n",
    "            line = \"C1 \" + line.lstrip()\n",
    "\n",
    "        modified_lines.append(line)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.writelines(modified_lines)\n",
    "\n",
    "modify_c1_lines('03_updated_WOS_1.txt', '04_C1updated_WOS_1.txt')\n",
    "modify_c1_lines('03_updated_WOS_2.txt', '04_C1updated_WOS_2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Check if there are any formatting errors in all C1 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = ['04_C1updated_WOS_1.txt', '04_C1updated_WOS_2.txt']\n",
    "\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "        for line_number, line in enumerate(file, 1):\n",
    "            if line.startswith('C1 ') and '[' not in line:\n",
    "                print(f\"In file {file_path}, line {line_number}: {line.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 Correcting Error in C1 Field Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Manually modify the final txt citation files: '05_final_WOS_1.txt', '05_final_WOS_2.txt' \n",
    "- Specifically, add a new line in the C1 field of the article \"Select Rab GTPases Regulate the Pulmonary Endothelium via Endosomal Trafficking of Vascular Endothelial-Cadherin\": \"Anglia Ruskin University, Biomedical Research Group, Department of Life Sciences, Faculty of Science and Technology, OPT 024, East Road, Cambridge CB1 1PT, England, United Kingdom.\" \n",
    "- Manually modify the citation files for convenient analysis of institution and country data using Python code: '05_C1_WOS_1.txt', '05_C1_WOS_2.txt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert a new line into the file 04_C1updated_WOS_2.txt.\n",
    "with open(\"04_C1updated_WOS_2.txt\", 'r', encoding='utf-8-sig') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "new_line = \"C1 Anglia Ruskin Univ, Biomed Res Grp, Dept Life Sci, Fac Sci & Technol, OPT 024,East Rd, Cambridge CB1 1PT, England, United Kingdom.\\n\"\n",
    "lines.insert(19691, new_line)\n",
    "\n",
    "with open(\"05_C1_WOS_2.txt\", 'w', encoding='utf-8-sig') as file:\n",
    "    file.writelines(lines)\n",
    "\n",
    "# Copy the content of file 04_C1updated_WOS_1.txt to 05_C1_WOS_1.txt.\n",
    "with open(\"04_C1updated_WOS_1.txt\", 'r', encoding='utf-8-sig') as file:\n",
    "    content = file.read()\n",
    "\n",
    "with open(\"05_C1_WOS_1.txt\", 'w', encoding='utf-8-sig') as file:\n",
    "    file.write(content)\n",
    "\n",
    "print(\"Files saved as 05_C1_WOS_1.txt and 05_C1_WOS_2.txt.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Statistical Analysis: Number of Publications, Citation Frequency, h-index, and Other Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The annual number of publications and the annual citation frequency can be obtained through the citation report on the web version of the WOS database.\n",
    "- The annual citation frequency cannot be obtained through a text citation file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 National Influence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Total Number of Publications and Multinational Publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = ['05_C1_WOS_1.txt', '05_C1_WOS_2.txt']\n",
    "\n",
    "# Create a dictionary to store the collaboration information of each country.\n",
    "country_publications = defaultdict(int)\n",
    "country_multinational_publications = defaultdict(int)\n",
    "\n",
    "for file_path in sorted(file_paths):\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "        countries = set()  # Used to store the country information for each article.\n",
    "        for line in file:\n",
    "            if line.startswith('C1 '):\n",
    "                country = line.strip().rsplit(',', 1)[-1].strip() \n",
    "                if country.endswith('.'):\n",
    "                    country = country[:-1]\n",
    "                countries.add(country)\n",
    "                # print(f\"Found country: {country}\") \n",
    "\n",
    "            # When an article comes to an end\n",
    "            if line.startswith('ER'):\n",
    "                # print(f\"Ending record with countries: {countries}\")  \n",
    "                if len(countries) > 1:  # If there are multiple countries, it will be a collaborative article.\n",
    "                    for country in countries:\n",
    "                        country_multinational_publications[country] += 1\n",
    "                for country in countries:\n",
    "                    country_publications[country] += 1\n",
    "                countries.clear()\n",
    "\n",
    "# Output Result\n",
    "with open('country_publications.csv', 'w', newline='', encoding='utf-8-sig') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Country', 'Publications', 'Multinational Publications'])\n",
    "    sorted_countries = sorted(country_publications, key=lambda country: country_publications[country], reverse=True)\n",
    "    for country in sorted_countries:\n",
    "        writer.writerow([country, country_publications[country], country_multinational_publications[country]])\n",
    "\n",
    "print(\"Data saved to 'country_publications.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Total Citations and h-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = ['05_C1_WOS_1.txt', '05_C1_WOS_2.txt']\n",
    "country_citations = defaultdict(int)\n",
    "country_citation_list = defaultdict(list)\n",
    "\n",
    "def calculate_h_index(citations):\n",
    "    citations.sort(reverse=True)\n",
    "    h_index = 0\n",
    "    for i in range(len(citations)):\n",
    "        if citations[i] >= i + 1:\n",
    "            h_index = i + 1\n",
    "        else:\n",
    "            break\n",
    "    return h_index\n",
    "\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "        countries = set()\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "\n",
    "            # Extract country information from row C1 and add it to the collection of countries.\n",
    "            if line.startswith('C1 '):\n",
    "                country = line.rsplit(',', 1)[-1].strip()\n",
    "                if country.endswith('.'):\n",
    "                    country = country[:-1]\n",
    "                countries.add(country)\n",
    "\n",
    "            # If a TC row is found, add the citation count to the corresponding country's citation frequency.\n",
    "            if line.startswith('TC '):\n",
    "                try:\n",
    "                    citation_count = int(line[3:].strip())\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping unexpected line: {line}\")\n",
    "                    continue\n",
    "                for country in countries:\n",
    "                    country_citations[country] += citation_count\n",
    "                    country_citation_list[country].append(citation_count)\n",
    "                countries.clear()\n",
    "\n",
    "# Calculate the h-index for each country and sort them.\n",
    "sorted_country_data = []\n",
    "for country, citations in country_citations.items():\n",
    "    h_index = calculate_h_index(country_citation_list[country])\n",
    "    sorted_country_data.append((country, citations, h_index))\n",
    "\n",
    "sorted_country_data.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Save the results to a CSV file.\n",
    "with open('country_citations_h-index.csv', 'w', newline='', encoding='utf-8-sig') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"Country\", \"Citations\", \"h-index\"])  \n",
    "    for row in sorted_country_data:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"Data saved to 'country_citations_h-index.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 number_of_cooperative_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = ['05_C1_WOS_1.txt', '05_C1_WOS_2.txt']\n",
    "country_cooperative_countries = defaultdict(set)\n",
    "all_countries = set()\n",
    "\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "        countries = set()\n",
    "        for line in file:\n",
    "            # Find national information.\n",
    "            if line.startswith('C1 '):\n",
    "                country_part = line.split(\",\")[-1].strip()\n",
    "                if country_part.endswith('.'):\n",
    "                    country_part = country_part[:-1]\n",
    "                country = \" \".join(country_part.split(\" \")[-2:])\n",
    "                if country:\n",
    "                    countries.add(country)\n",
    "                    all_countries.add(country) # Storing all found countries\n",
    "                # print(f\"Found country: {country}\")\n",
    "\n",
    "            # When an article ends, add the cooperative relationships between countries to the dictionary \"country_cooperative_countries\".\n",
    "            if line.startswith('ER'):\n",
    "                if len(countries) > 1:\n",
    "                    for country in countries:\n",
    "                        country_cooperative_countries[country].update(countries - {country})\n",
    "                countries.clear()\n",
    "\n",
    "# Calculate the number of collaborating countries for each country.\n",
    "country_number_of_cooperative_countries = {country: len(country_cooperative_countries.get(country, set())) for country in all_countries}\n",
    "\n",
    "# Output Result\n",
    "with open('country_number_of_cooperative_countries.csv', 'w', newline='', encoding='utf-8-sig') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Country', 'Number of Cooperative Countries'])\n",
    "    for country, number_of_cooperative_countries in country_number_of_cooperative_countries.items():\n",
    "        writer.writerow([country, number_of_cooperative_countries])\n",
    "\n",
    "print(\"Calculation completed. Data saved to 'country_number_of_cooperative_countries.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 Merging National Data Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read each CSV file.\n",
    "h_indices_df = pd.read_csv('country_citations_h-index.csv')\n",
    "cooperative_countries_df = pd.read_csv('country_number_of_cooperative_countries.csv')\n",
    "publications_df = pd.read_csv('country_publications.csv')\n",
    "\n",
    "# Merge these DataFrames using the \"Country\" column.\n",
    "merged_df = h_indices_df.merge(cooperative_countries_df, on='Country', how='outer')\n",
    "merged_df = merged_df.merge(publications_df, on='Country', how='outer')\n",
    "\n",
    "# Fill in NaN values (if any).\n",
    "merged_df.fillna(0, inplace=True)\n",
    "\n",
    "# Calculate the column Average Citations per Publication.\n",
    "merged_df['Average Citations per Publication'] = (merged_df['Citations'] / merged_df['Publications']).round(2)\n",
    "\n",
    "# Calculate the share of publications from multinational collaborations.\n",
    "merged_df['share of multinational cooperation publications'] = (merged_df['Multinational Publications'] / merged_df['Publications'] * 100).round(2)\n",
    "\n",
    "# Reorganize according to the desired column order.\n",
    "columns_order = [\n",
    "    'Country', 'Publications', 'Citations', 'h-index',\n",
    "    'Average Citations per Publication', 'Number of Cooperative Countries',\n",
    "    'Multinational Publications', 'share of multinational cooperation publications'\n",
    "]\n",
    "merged_df = merged_df[columns_order]\n",
    "\n",
    "# Sort in descending order by the columns 'Publications', 'Average Citations per Publication', and 'h-index'.\n",
    "merged_df = merged_df.sort_values(by=['Publications', 'Average Citations per Publication', 'h-index'], ascending=False)\n",
    "\n",
    "# Save to a new CSV file.\n",
    "merged_df.to_csv('Data_Country.csv', index=False)\n",
    "\n",
    "print(\"Files have been successfully merged and saved to 'Data_Country.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 institution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Total Publications, Citations, h-index and Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = ['05_C1_WOS_1.txt', '05_C1_WOS_2.txt']\n",
    "institution_publications = defaultdict(int)\n",
    "institution_citations = defaultdict(int)\n",
    "institution_citation_list = defaultdict(list)\n",
    "institution_country = {}\n",
    "\n",
    "def calculate_h_index(citations):\n",
    "    citations.sort(reverse=True)\n",
    "    h_index = 0\n",
    "    for i in range(len(citations)):\n",
    "        if citations[i] >= i + 1:\n",
    "            h_index = i + 1\n",
    "        else:\n",
    "            break\n",
    "    return h_index\n",
    "\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "        institutions = set()\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "\n",
    "            # Extract institution and country information from line C1 and add it to the institution collection.\n",
    "            if line.startswith('C1 '):\n",
    "                if ']' in line:\n",
    "                    institution = line.split(']')[-1].split(',')[0].strip()\n",
    "                else:\n",
    "                    institution = line.split('C1 ')[-1].split(',')[0].strip()\n",
    "\n",
    "                # Extracting National Information\n",
    "                country = line.split(\",\")[-1].strip()\n",
    "                if country.endswith('.'):\n",
    "                    country = country[:-1]\n",
    "\n",
    "                institutions.add(institution)\n",
    "                if institution not in institution_country:\n",
    "                    institution_country[institution] = country\n",
    "\n",
    "            # If a TC line is found, add the number of citations to the corresponding institution's citation frequency and update the institution's publication count.\n",
    "            if line.startswith('TC '):\n",
    "                try:\n",
    "                    citation_count = int(line[3:].strip())\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping unexpected line: {line}\")\n",
    "                    continue\n",
    "                for institution in institutions:\n",
    "                    institution_citations[institution] += citation_count\n",
    "                    institution_publications[institution] += 1\n",
    "                    institution_citation_list[institution].append(citation_count)\n",
    "                institutions.clear()\n",
    "\n",
    "# Calculate the h-index for each institution and sort them.\n",
    "sorted_institution_data = []\n",
    "for institution, count in institution_publications.items():\n",
    "    h_index = calculate_h_index(institution_citation_list[institution])\n",
    "    sorted_institution_data.append((institution, count, institution_citations[institution], h_index))\n",
    "\n",
    "sorted_institution_data.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Save the results to a CSV file.\n",
    "with open('institution_data.csv', 'w', newline='', encoding='utf-8-sig') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"Institution\", \"Country\", \"Publications\", \"Citations\", \"h-index\"])  # 添加国家列\n",
    "    for institution, count in institution_publications.items():\n",
    "        country = institution_country.get(institution, \"Unknown\")\n",
    "        h_index = calculate_h_index(institution_citation_list[institution])\n",
    "        writer.writerow([institution, country, count, institution_citations[institution], h_index])\n",
    "\n",
    "print(\"Data saved to 'institution_data.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Institutional Data Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_publications = 972\n",
    "\n",
    "with open('institution_data.csv', 'r', encoding='utf-8-sig') as infile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    data = [row for row in reader]\n",
    "\n",
    "# Processing data, adding new columns.\n",
    "for row in data:\n",
    "    publications = int(row['Publications'])\n",
    "    citations = int(row['Citations'])\n",
    "    row['Average Citations per Publication'] = round(citations / publications, 2)\n",
    "    row['the percentage of articles of institutions in total publications'] = round((publications / total_publications) * 100, 2)\n",
    "\n",
    "# Define a new column name.\n",
    "fieldnames = [\n",
    "    'Institution', 'Publications', 'Citations', 'h-index', 'Average Citations per Publication',\n",
    "    'the percentage of articles of institutions in total publications', 'Country'\n",
    "]\n",
    "\n",
    "# Sort in descending order according to the desired column.\n",
    "sorted_data = sorted(data, key=lambda x: (\n",
    "    int(x['Publications']), float(x['Average Citations per Publication']), int(x['h-index'])\n",
    "), reverse=True)\n",
    "\n",
    "# Write the updated data into a new CSV file.\n",
    "with open('Data_Institution.csv', 'w', newline='', encoding='utf-8-sig') as outfile:\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(sorted_data)\n",
    "\n",
    "print(\"New columns added and sorted successfully. Data saved to 'Data_Institution.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Analysis of Author's Influence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Total Number of Publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file paths\n",
    "file_paths = [\"05_final_WOS_1.txt\", \"05_final_WOS_2.txt\"]\n",
    "\n",
    "# Initialize a list to collect authors\n",
    "authors_from_af_corrected_multiline = []\n",
    "\n",
    "# Loop through the files and extract authors from the AF field\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r', encoding='utf-8-sig', errors='ignore') as file:\n",
    "        in_af_field = False\n",
    "        for line in file:\n",
    "            if line.startswith(\"AF\"):\n",
    "                in_af_field = True\n",
    "                authors_from_af_corrected_multiline.append(line[3:].strip())\n",
    "            elif in_af_field and line.startswith(\" \"):\n",
    "                authors_from_af_corrected_multiline.append(line.strip())\n",
    "            elif in_af_field and not line.startswith(\" \"):\n",
    "                # Exiting the AF field\n",
    "                in_af_field = False\n",
    "\n",
    "# Counting the publication count for each author\n",
    "author_publication_count_af_multiline = Counter(authors_from_af_corrected_multiline)\n",
    "\n",
    "# Creating a DataFrame from the author publication counts\n",
    "author_df_af_multiline = pd.DataFrame(list(author_publication_count_af_multiline.items()), columns=['Author', 'Publication Count'])\n",
    "\n",
    "# Sorting by publication count\n",
    "author_df_af_multiline = author_df_af_multiline.sort_values(by='Publication Count', ascending=False)\n",
    "\n",
    "# Exporting to CSV\n",
    "csv_file_path_af_multiline = \"author_publications.csv\"\n",
    "author_df_af_multiline.to_csv(csv_file_path_af_multiline, index=False)\n",
    "\n",
    "print(f\"CSV file exported to {csv_file_path_af_multiline}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Total Citation Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of File Names\n",
    "filenames = [\"05_final_WOS_1.txt\", \"05_final_WOS_2.txt\"]\n",
    "\n",
    "# Dictionary used to store the author and their total citation count.\n",
    "author_citation_counts = {}\n",
    "\n",
    "\n",
    "# Loop through each file.\n",
    "for filename in filenames:\n",
    "    authors = []  # Store a list of authors for an article.\n",
    "    in_af_field = False # Indicate whether it is within the AF field.\n",
    "    with open(filename, 'r', encoding='utf-8-sig') as file:\n",
    "        for line in file:\n",
    "            # If in the AF field\n",
    "            if line.startswith('AF'):\n",
    "                authors.append(line[3:].strip())\n",
    "                in_af_field = True\n",
    "            elif in_af_field and line.strip() and line[0] == ' ':\n",
    "                authors.append(line.strip())\n",
    "            else:\n",
    "                in_af_field = False # Not in the AF field.\n",
    "\n",
    "            # If in the TC field\n",
    "            if line.startswith('TC'):\n",
    "                citation_count = int(line[3:].strip())\n",
    "                for author in authors:\n",
    "                    author_citation_counts[author] = author_citation_counts.get(author, 0) + citation_count\n",
    "                authors = []  # Reset the list of authors for the next article.\n",
    "\n",
    "# Print the author and their total citation count.\n",
    "# for author, citations in author_citation_counts.items():\n",
    "    # print(f\"{author}: {citations} citations\")\n",
    "\n",
    "with open('author_citations.csv', 'w', encoding='utf-8-sig', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Author', 'Citations'])\n",
    "    for author, citations in author_citation_counts.items():\n",
    "        writer.writerow([author, citations])\n",
    "\n",
    "print(\"Data saved to 'author_citations.csv'.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 h-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['05_final_WOS_1.txt', '05_final_WOS_2.txt']\n",
    "\n",
    "# Store a list of citation counts for each author.\n",
    "author_citations = defaultdict(list)\n",
    "\n",
    "# Read the file line by line.\n",
    "for filename in filenames:\n",
    "    authors = []  # Store the authors of the current paper.\n",
    "    in_af_field = False # True if the current line is in the AF field.\n",
    "    with open(filename, 'r', encoding='utf-8-sig') as file:\n",
    "        for line in file:\n",
    "            # IF the current line is in the AF field, append the author to the list.\n",
    "            if line.startswith('AF'):\n",
    "                authors.append(line[3:].strip())\n",
    "                in_af_field = True\n",
    "            elif in_af_field and line.strip() and line[0] == ' ':\n",
    "                authors.append(line.strip())\n",
    "            else:\n",
    "                in_af_field = False \n",
    "\n",
    "            # IF the current line is in the TC field, append the citation count to the list.\n",
    "            if line.startswith('TC'):\n",
    "                citation_count = int(line[3:].strip())\n",
    "                for author in authors:\n",
    "                    author_citations[author].append(citation_count)\n",
    "                authors = []  # Reset the list of authors.\n",
    "\n",
    "#  Calculate the h-index for each author.\n",
    "author_h_indexes = {}\n",
    "for author, citations in author_citations.items():\n",
    "    citations.sort(reverse=True)\n",
    "    h_index = 0\n",
    "    for i, citation in enumerate(citations):\n",
    "        if citation >= i + 1:\n",
    "            h_index = i + 1\n",
    "        else:\n",
    "            break\n",
    "    author_h_indexes[author] = h_index\n",
    "\n",
    "# Write the author h-indexes to a CSV file.\n",
    "with open('author_h_index.csv', 'w', encoding='utf-8-sig', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Author', 'h-index'])\n",
    "    for author, h_index in author_h_indexes.items():\n",
    "        writer.writerow([author, h_index])\n",
    "\n",
    "print(\"Data saved to 'author_h_index.csv'.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 Obtain the author's latest affiliation with an institution or country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_latest_institution_country(file_path):\n",
    "    author_institution = {}\n",
    "    author_country = {}\n",
    "    # Read the file line by line\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\"C1\"):\n",
    "                match = re.match(r\"C1 \\[(.*?)\\](.*)\", line)\n",
    "                if match:\n",
    "                    authors = [author.strip() for author in match.group(1).split(\";\")]\n",
    "                    institution_country_info = match.group(2)\n",
    "                    institution_parts = institution_country_info.split(\",\")\n",
    "                    institution = institution_parts[0].strip()\n",
    "                    country = institution_parts[-1].strip()\n",
    "                    if country.endswith('.'):\n",
    "                        country = country[:-1]\n",
    "                    for author in authors:\n",
    "                        if author and author not in author_institution:\n",
    "                            author_institution[author] = institution\n",
    "                            author_country[author] = country\n",
    "    # Return the extracted data\n",
    "    return author_institution, author_country\n",
    "\n",
    "# Read the replacefile_authornames.csv and build a dictionary to hold the replacements\n",
    "replacements = {}\n",
    "with open('replacefile_authornames.csv', 'r', encoding='utf-8-sig') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # skip the header row\n",
    "    for row in reader:\n",
    "        original_name, replace_by = row\n",
    "        replacements[original_name] = replace_by\n",
    "# Extract the latest institution and country for each author\n",
    "file_paths = ['05_C1_WOS_1.txt', '05_C1_WOS_2.txt']\n",
    "all_author_institution = {}\n",
    "all_author_country = {}\n",
    "# Loop through all the files and extract the data\n",
    "for file_path in file_paths:\n",
    "    author_institution, author_country = extract_latest_institution_country(file_path)\n",
    "    all_author_institution.update(author_institution)\n",
    "    all_author_country.update(author_country)\n",
    "# Write the extracted data to a CSV file\n",
    "with open('authors_institutions_countries.csv', 'w', newline='', encoding='utf-8-sig') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"Author\", \"Institution\", \"Country\"])\n",
    "    seen_authors = set()\n",
    "    for author in all_author_institution:\n",
    "        # Apply replacements if needed\n",
    "        final_author_name = replacements.get(author, author)\n",
    "        if final_author_name not in seen_authors:\n",
    "            seen_authors.add(final_author_name)\n",
    "            country = all_author_country[author]\n",
    "            writer.writerow([final_author_name, all_author_institution[author], country])\n",
    "\n",
    "print(\"Extraction completed. Data saved to 'authors_institutions_countries.csv'.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.5 Merging Author Influence Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read four CSV files\n",
    "citations_df = pd.read_csv('author_citations.csv')\n",
    "h_index_df = pd.read_csv('author_h_index.csv')\n",
    "publications_df = pd.read_csv('author_publications.csv')\n",
    "authors_institutions_countries_df = pd.read_csv('authors_institutions_countries.csv')\n",
    "\n",
    "# Merge dataframes based on 'Author' column, use 'inner' to only keep authors found in 'author_citations.csv'\n",
    "merged_df = pd.merge(citations_df, h_index_df, on='Author', how='left')\n",
    "merged_df = pd.merge(merged_df, publications_df, on='Author', how='left')\n",
    "merged_df = pd.merge(merged_df, authors_institutions_countries_df, on='Author', how='left')\n",
    "\n",
    "# Calculate average citation value, keep two decimal places\n",
    "merged_df['Average Citations per Publication'] = (merged_df['Citations'] / merged_df['Publication Count']).round(2)\n",
    "\n",
    "# Rename column names\n",
    "merged_df.rename(columns={'Publication Count': 'Publications', 'h-index': 'h-index'}, inplace=True)\n",
    "\n",
    "# Rearrange the columns in the required order\n",
    "merged_df = merged_df[['Author', 'Publications', 'Citations', 'h-index', 'Average Citations per Publication', 'Institution', 'Country']]\n",
    "\n",
    "# Sort by 'Publications', 'Average Citations per Publication', and 'h-index' columns in descending order\n",
    "merged_df = merged_df.sort_values(by=['Publications', 'Average Citations per Publication', 'h-index'], ascending=[False, False, False])\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "merged_df.to_csv('Data_Author.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"Data saved to 'Data_Author.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Journal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Publications, Citations, ACPP, and TPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\"05_final_WOS_1.txt\", \"05_final_WOS_2.txt\"]\n",
    "journal_publications = defaultdict(int)\n",
    "journal_citations = defaultdict(int)\n",
    "total_publications = 972\n",
    "\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "        current_journal = None\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "\n",
    "            if line.startswith('SO '):\n",
    "                current_journal = line[3:].strip()\n",
    "\n",
    "            if line.startswith('TC ') and current_journal:\n",
    "                try:\n",
    "                    citation_count = int(line[3:].strip())\n",
    "                    journal_citations[current_journal] += citation_count\n",
    "                    journal_publications[current_journal] += 1\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping unexpected line: {line}\")\n",
    "                    continue\n",
    "                current_journal = None\n",
    "\n",
    "# Preparing data.\n",
    "data = []\n",
    "for journal, publications in journal_publications.items():\n",
    "    citations = journal_citations[journal]\n",
    "    avg_citations_per_publication = round(citations / publications, 2)\n",
    "    percentage_of_articles = round((publications / total_publications) * 100, 2)\n",
    "    data.append((journal, publications, citations, avg_citations_per_publication, percentage_of_articles))\n",
    "\n",
    "# Sort in descending order by the number of posts.\n",
    "sorted_data = sorted(data, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Save the sorted results to a CSV file.\n",
    "with open('Data_Journal.csv', 'w', newline='', encoding='utf-8-sig') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"Journal\", \"Publications\", \"Citations\", \"Average Citations per Publication\",\n",
    "                     \"the percentage of articles of institutions in total publications\"])\n",
    "    writer.writerows(sorted_data)\n",
    "\n",
    "print(\"Data saved to 'Data_Journal.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Bubble_journal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = ['05_final_WOS_1.txt', '05_final_WOS_2.txt']\n",
    "journal_publications = defaultdict(int)\n",
    "journal_yearly_publications = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Save all occurrence years\n",
    "all_years = set()\n",
    "\n",
    "# Journal used for storing current literature.\n",
    "current_journal = ''\n",
    "\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "\n",
    "            # Read the SO line and save the journal.\n",
    "            if line.startswith('SO '):\n",
    "                current_journal = line[3:].strip()\n",
    "\n",
    "            # Read the PY line, associated with the current journal\n",
    "            elif line.startswith('PY '):\n",
    "                try:\n",
    "                    publication_year = int(line[3:].strip())\n",
    "                    all_years.add(publication_year)\n",
    "\n",
    "                    # If there are journals, conduct statistics.\n",
    "                    if current_journal:\n",
    "                        journal_publications[current_journal] += 1\n",
    "                        journal_yearly_publications[current_journal][publication_year] += 1\n",
    "\n",
    "                    # Clear the current journal and prepare to process the next document.\n",
    "                    current_journal = ''\n",
    "\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping unexpected line: {line}\")\n",
    "                    continue\n",
    "\n",
    "all_years = sorted(list(all_years))\n",
    "\n",
    "# Create a list to store journals and their corresponding data.\n",
    "journals_data = []\n",
    "\n",
    "# Save data to a list.\n",
    "for journal, total_count in journal_publications.items():\n",
    "    row = [journal, total_count] + [journal_yearly_publications[journal][year] for year in all_years]\n",
    "    journals_data.append(row)\n",
    "\n",
    "# Sort in descending order based on the value of the \"total publications\" column.\n",
    "journals_data.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Save the sorted results\n",
    "with open('Bubble_journals_data.csv', 'w', newline='', encoding='utf-8-sig') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    header = [\"journal\", \"total publications\"] + [str(year) for year in all_years]\n",
    "    writer.writerow(header)\n",
    "    for row in journals_data:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"Data saved to 'Bubble_journals_data.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 Bubble Chart of Top 20 journal by Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the global font to Times New Roman.\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"font.weight\"] = \"bold\"\n",
    "\n",
    "# Read CSV file.\n",
    "file_path = 'Bubble_journals_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Top 20 research fields by total number of publications.\n",
    "top_30_keywords = df.nlargest(20, 'total publications')[::-1]\n",
    "\n",
    "# Define the color.\n",
    "colors = sns.color_palette(\"tab20\", 30)\n",
    "\n",
    "plt.figure(figsize=[10, 10])\n",
    "\n",
    "plt.xticks(fontsize=14, fontweight='bold', fontname='Times New Roman')\n",
    "plt.yticks(fontsize=10, fontweight='bold', fontname='Times New Roman')\n",
    "\n",
    "# Hide the y-axis tick marks.\n",
    "plt.tick_params(axis='y', which='both', left=False)\n",
    "plt.tick_params(axis='x', which='both', bottom=False, top=False)\n",
    "\n",
    "# Add gridlines.\n",
    "plt.grid(axis='y', linestyle='-', linewidth=0.5, color='#2E2E2E')\n",
    "plt.grid(axis='x', linestyle='--', linewidth=0.5, color='lightgray')\n",
    "\n",
    "# Hide the outer border lines on both sides.\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Creating Bubble Chart\n",
    "for index, row in top_30_keywords.iterrows():\n",
    "    keyword = row['journal']\n",
    "    yearly_data = row[2:].values\n",
    "    years = top_30_keywords.columns[2:]\n",
    "\n",
    "    sizes = yearly_data.astype(int) * 50\n",
    "    color = colors[index % 30]\n",
    "\n",
    "    plt.scatter(years, [keyword]*len(years), s=sizes, c=[color]*len(years), alpha=0.8, edgecolors='w', zorder=3)\n",
    "    \n",
    "    for year, publications, size in zip(years, yearly_data, sizes):\n",
    "        if publications > 0:\n",
    "            plt.text(year, keyword, int(publications), ha='center', va='center', color='black')\n",
    "\n",
    "# Display the scale labels at the top.\n",
    "plt.tick_params(labeltop=True)\n",
    "\n",
    "# Hide axis labels.\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "\n",
    "# Customize the y-axis scale to accommodate long labels.\n",
    "custom_ticks = top_30_keywords['journal'].apply(lambda x: textwrap.fill(x, 40))\n",
    "plt.yticks(ticks=top_30_keywords['journal'].values, labels=custom_ticks)\n",
    "\n",
    "# Save the image.\n",
    "if not os.path.exists('pictures'):\n",
    "    os.makedirs('pictures')\n",
    "plt.tight_layout()\n",
    "plt.savefig('pictures/Bubble_Chart_journal.svg', format='svg')\n",
    "\n",
    "# Display the image.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 WOS research area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 Processing WC field lines in txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\"05_final_WOS_1.txt\", \"05_final_WOS_2.txt\"]\n",
    "\n",
    "def process_file(input_file_path, output_file_path):\n",
    "    with open(input_file_path, 'r', encoding='utf-8-sig') as infile, open(output_file_path, 'w', encoding='utf-8-sig') as outfile:\n",
    "        appending = False\n",
    "        current_line = ''\n",
    "        for line in infile:\n",
    "            stripped_line = line.strip()\n",
    "            if stripped_line.startswith('WC '):\n",
    "                current_line = stripped_line\n",
    "                appending = True\n",
    "                continue\n",
    "            if appending and line.startswith(' '):\n",
    "                current_line += ' ' + stripped_line\n",
    "                continue\n",
    "            if appending:\n",
    "                appending = False\n",
    "                outfile.write(current_line + '\\n')\n",
    "                current_line = ''\n",
    "            outfile.write(line)\n",
    "\n",
    "for index, file_path in enumerate(file_paths):\n",
    "    process_file(file_path, f\"processed_{index}.txt\")\n",
    "\n",
    "print(\"Files processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 Publications, Citations, and h-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_publications = 972\n",
    "file_paths = ['processed_0.txt', 'processed_1.txt']\n",
    "research_field_publications = defaultdict(int)\n",
    "research_field_citations = defaultdict(int)\n",
    "research_field_citation_list = defaultdict(list)\n",
    "\n",
    "def calculate_h_index(citations):\n",
    "    citations.sort(reverse=True)\n",
    "    h_index = 0\n",
    "    for i in range(len(citations)):\n",
    "        if citations[i] >= i + 1:\n",
    "            h_index = i + 1\n",
    "        else:\n",
    "            break\n",
    "    return h_index\n",
    "\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "        citation_count = 0\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "\n",
    "            if line.startswith('TC '):\n",
    "                try:\n",
    "                    citation_count = int(line[3:].strip())\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping unexpected line: {line}\")\n",
    "                    continue\n",
    "\n",
    "            if line.startswith('WC '):\n",
    "                research_fields = line[3:].strip().split(';')\n",
    "                for field in research_fields:\n",
    "                    field = field.strip()\n",
    "                    research_field_citations[field] += citation_count\n",
    "                    research_field_publications[field] += 1\n",
    "                    research_field_citation_list[field].append(citation_count)\n",
    "\n",
    "research_field_data = []\n",
    "for field, count in research_field_publications.items():\n",
    "    h_index = calculate_h_index(research_field_citation_list[field])\n",
    "    avg_citations = round(research_field_citations[field] / count, 2)\n",
    "    percentage_of_articles = round((count / total_publications) * 100, 2)\n",
    "    research_field_data.append((field, count, research_field_citations[field], h_index, avg_citations, percentage_of_articles))\n",
    "\n",
    "research_field_data.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "with open('Data_Research_Area.csv', 'w', newline='', encoding='utf-8-sig') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"Research Field\", \"Publications\", \"Citations\", \"h-index\", \"Average Citations per Publication\", \"the percentage of articles of institutions in total publications\"])\n",
    "    for row in research_field_data:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"Data saved to 'Data_Research_Area.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.3 Publications of WOS research area by Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = ['processed_0.txt', 'processed_1.txt']\n",
    "fields_publications = defaultdict(int)\n",
    "fields_yearly_publications = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "all_years = set()\n",
    "current_year = None  # Year used to save the current document.\n",
    "\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Read the PY line and save the year of the current document.\n",
    "            if line.startswith('PY '):\n",
    "                try:\n",
    "                    current_year = int(line[3:].strip())\n",
    "                    all_years.add(current_year)\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping unexpected line: {line}\")\n",
    "                    continue\n",
    "\n",
    "            # Read WC lines and save research areas.\n",
    "            elif line.startswith('WC '):\n",
    "                fields = line[3:].strip().split(';')\n",
    "                fields = [field.strip() for field in fields]\n",
    "                for field in fields:\n",
    "                    if current_year is not None:\n",
    "                        fields_publications[field] += 1\n",
    "                        fields_yearly_publications[field][current_year] += 1\n",
    "            \n",
    "            # Read the ER line and clear the current document's year.\n",
    "            elif line.startswith('ER '):\n",
    "                current_year = None\n",
    "\n",
    "all_years = sorted(list(all_years))\n",
    "\n",
    "fields_data = []\n",
    "\n",
    "for field, total_count in fields_publications.items():\n",
    "    row = [field, total_count] + [fields_yearly_publications[field][year] for year in all_years]\n",
    "    fields_data.append(row)\n",
    "\n",
    "fields_data.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "with open('Bubble_fields_data.csv', 'w', newline='', encoding='utf-8-sig') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    header = [\"researchfields\", \"total publications\"] + [str(year) for year in all_years]\n",
    "    writer.writerow(header)\n",
    "    for row in fields_data:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"Data saved to 'Bubble_fields_data.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.4 Bubble Chart of Top 20 WOS research area by Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the global font to Times New Roman.\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"font.weight\"] = \"bold\"\n",
    "\n",
    "# Read CSV file.\n",
    "file_path = 'Bubble_fields_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Top 20 research fields by total number of publications.\n",
    "top_30_keywords = df.nlargest(20, 'total publications')[::-1]\n",
    "\n",
    "# Define the color.\n",
    "colors = sns.color_palette(\"tab20\", 30)\n",
    "\n",
    "plt.figure(figsize=[10, 10])\n",
    "\n",
    "plt.xticks(fontsize=14, fontweight='bold', fontname='Times New Roman')\n",
    "plt.yticks(fontsize=10, fontweight='bold', fontname='Times New Roman')\n",
    "\n",
    "# Hide the y-axis tick marks.\n",
    "plt.tick_params(axis='y', which='both', left=False)\n",
    "plt.tick_params(axis='x', which='both', bottom=False, top=False)\n",
    "\n",
    "# Add gridlines.\n",
    "plt.grid(axis='y', linestyle='-', linewidth=0.5, color='#2E2E2E')\n",
    "plt.grid(axis='x', linestyle='--', linewidth=0.5, color='lightgray')\n",
    "\n",
    "# Hide the outer border lines on both sides.\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Creating Bubble Chart\n",
    "for index, row in top_30_keywords.iterrows():\n",
    "    keyword = row['researchfields']\n",
    "    yearly_data = row[2:].values\n",
    "    years = top_30_keywords.columns[2:]\n",
    "\n",
    "    sizes = yearly_data.astype(int) * 50\n",
    "    color = colors[index % 30]\n",
    "\n",
    "    plt.scatter(years, [keyword]*len(years), s=sizes, c=[color]*len(years), alpha=0.8, edgecolors='w', zorder=3)\n",
    "    \n",
    "    for year, publications, size in zip(years, yearly_data, sizes):\n",
    "        if publications > 0:\n",
    "            plt.text(year, keyword, int(publications), ha='center', va='center', color='black')\n",
    "\n",
    "# Display the scale labels at the top.\n",
    "plt.tick_params(labeltop=True)\n",
    "\n",
    "# Hide axis labels.\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "\n",
    "# Customize the y-axis scale to accommodate long labels.\n",
    "custom_ticks = top_30_keywords['researchfields'].apply(lambda x: textwrap.fill(x, 20))\n",
    "plt.yticks(ticks=top_30_keywords['researchfields'].values, labels=custom_ticks)\n",
    "\n",
    "# Save the image.\n",
    "if not os.path.exists('pictures'):\n",
    "    os.makedirs('pictures')\n",
    "plt.tight_layout()\n",
    "plt.savefig('pictures/Bubble_Chart_fields.svg', format='svg')\n",
    "\n",
    "# Display the image.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Author Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.1 Retrieve all documents that do not contain the field \"DE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = ['05_final_WOS_1.txt', '05_final_WOS_2.txt']\n",
    "output_file_path = 'without_DE_records.txt'\n",
    "\n",
    "count_without_DE = 0\n",
    "\n",
    "with open(output_file_path, 'w', encoding='utf-8-sig') as outfile:\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8-sig') as infile:\n",
    "            inside_record = False\n",
    "            has_DE = False\n",
    "            for line in infile:\n",
    "                if line.startswith('PT '):\n",
    "                    inside_record = True\n",
    "                    record_lines = [line]\n",
    "                    has_DE = False\n",
    "                elif inside_record and line.startswith('DE '):\n",
    "                    has_DE = True\n",
    "                elif inside_record and line.startswith('ER'):\n",
    "                    record_lines.append(line)\n",
    "                    inside_record = False\n",
    "                    if not has_DE:\n",
    "                        count_without_DE += 1\n",
    "                        outfile.writelines(record_lines)\n",
    "                        outfile.write('\\n')  \n",
    "                elif inside_record:\n",
    "                    record_lines.append(line)\n",
    "\n",
    "print(f\"Number of records without DE line: {count_without_DE}\")\n",
    "print(f\"Records without DE line exported to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.2 Publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = ['05_final_WOS_1.txt', '05_final_WOS_2.txt']\n",
    "keywords_publications = defaultdict(int)\n",
    "keywords_yearly_publications = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Save all the years that appear.\n",
    "all_years = set()\n",
    "\n",
    "# Keywords for saving the current literature.\n",
    "current_keywords = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "\n",
    "            # Read line DE and save keywords.\n",
    "            if line.startswith('DE '):\n",
    "                current_keywords = line[3:].strip().split(';')\n",
    "                current_keywords = [keyword.strip() for keyword in current_keywords]\n",
    "\n",
    "            # Read the PY line and associate it with the current keyword.\n",
    "            elif line.startswith('PY '):\n",
    "                try:\n",
    "                    publication_year = int(line[3:].strip())\n",
    "                    all_years.add(publication_year)\n",
    "\n",
    "                    # If there are keywords, perform a statistical analysis.\n",
    "                    for keyword in current_keywords:\n",
    "                        keywords_publications[keyword] += 1\n",
    "                        keywords_yearly_publications[keyword][publication_year] += 1\n",
    "\n",
    "                    # Clear the current keyword and prepare to process the next document.\n",
    "                    current_keywords = []\n",
    "\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping unexpected line: {line}\")\n",
    "                    continue\n",
    "\n",
    "all_years = sorted(list(all_years))\n",
    "\n",
    "# Create a list to store keywords and their corresponding data.\n",
    "keywords_data = []\n",
    "\n",
    "# Save data to a list.\n",
    "for keyword, total_count in keywords_publications.items():\n",
    "    row = [keyword, total_count] + [keywords_yearly_publications[keyword][year] for year in all_years]\n",
    "    keywords_data.append(row)\n",
    "\n",
    "# Sort in descending order based on the value of the \"total publications\" column.\n",
    "keywords_data.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Save the sorted results\n",
    "with open('Bubble_keywords_data.csv', 'w', newline='', encoding='utf-8-sig') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    header = [\"authorkeywords\", \"total publications\"] + [str(year) for year in all_years]\n",
    "    writer.writerow(header)\n",
    "    for row in keywords_data:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"Data saved to 'Bubble_keywords_data.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.3 Bubble Chart of Top 30 Author Keywords by Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the global font to Times New Roman.\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"font.weight\"] = \"bold\"\n",
    "\n",
    "# Read CSV file.\n",
    "file_path = 'Bubble_keywords_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Retrieve the keywords of the top 30 authors based on their total number of published articles.\n",
    "top_30_keywords = df.nlargest(30, 'total publications')[::-1]\n",
    "\n",
    "# Define the color.\n",
    "colors = sns.color_palette(\"tab20\", 30) \n",
    "\n",
    "plt.figure(figsize=[10, 15])\n",
    "\n",
    "plt.xticks(fontsize=14, fontweight='bold', fontname='Times New Roman')\n",
    "plt.yticks(fontsize=10, fontweight='bold', fontname='Times New Roman')\n",
    "\n",
    "# Hide the tick marks on the x and y axes.\n",
    "plt.tick_params(axis='y', which='both', left=False)\n",
    "plt.tick_params(axis='x', which='both', bottom=False, top=False)\n",
    "\n",
    "# Add gridlines.\n",
    "plt.grid(axis='y', linestyle='-', linewidth=0.5, color='#2E2E2E')\n",
    "plt.grid(axis='x', linestyle='--', linewidth=0.5, color='lightgray')\n",
    "\n",
    "# Hide the outer border lines on both sides.\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "plt.ylim(-0.5, len(top_30_keywords['authorkeywords']) - 0.3)\n",
    "\n",
    "# Define the maximum size of the bubble.\n",
    "MAX_BUBBLE_SIZE = 1300\n",
    "\n",
    "# Draw a bubble chart.\n",
    "for index, row in top_30_keywords.iterrows():\n",
    "    keyword = row['authorkeywords']\n",
    "    yearly_data = row[2:].values\n",
    "    years = top_30_keywords.columns[2:]\n",
    "\n",
    "    sizes = np.minimum(yearly_data.astype(int) * 50, MAX_BUBBLE_SIZE)  \n",
    "    color = colors[index % 30]\n",
    "\n",
    "    plt.scatter(years, [keyword]*len(years), s=sizes, c=[color]*len(years), alpha=0.8, edgecolors='w', zorder=3)    \n",
    "\n",
    "    for year, publications, size in zip(years, yearly_data, sizes):\n",
    "        if publications > 0:\n",
    "            plt.text(year, keyword, int(publications), ha='center', va='center', color='black')\n",
    "\n",
    "# Display the scale labels at the top.\n",
    "plt.tick_params(labeltop=True)\n",
    "\n",
    "# Hide axis labels.\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "\n",
    "# Customize the y-axis scale to accommodate long labels.\n",
    "custom_ticks = top_30_keywords['authorkeywords'].apply(lambda x: textwrap.fill(x, 40))\n",
    "plt.yticks(ticks=top_30_keywords['authorkeywords'].values, labels=custom_ticks)\n",
    "\n",
    "# Save the image.\n",
    "if not os.path.exists('pictures'):\n",
    "    os.makedirs('pictures')\n",
    "plt.tight_layout()\n",
    "plt.savefig('pictures/Bubble_Chart_AuthorKeywords.svg', format='svg')\n",
    "\n",
    "# Display the image.\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
